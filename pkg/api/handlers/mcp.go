package handlers

import (
	"context"
	"log"
	"sync"
	"time"

	"github.com/gofiber/fiber/v2"
	"github.com/kubestellar/console/pkg/k8s"
	"github.com/kubestellar/console/pkg/mcp"
)

// MCPHandlers handles MCP-related API endpoints
type MCPHandlers struct {
	bridge    *mcp.Bridge
	k8sClient *k8s.MultiClusterClient
}

// NewMCPHandlers creates a new MCP handlers instance
func NewMCPHandlers(bridge *mcp.Bridge, k8sClient *k8s.MultiClusterClient) *MCPHandlers {
	return &MCPHandlers{
		bridge:    bridge,
		k8sClient: k8sClient,
	}
}

// GetStatus returns the MCP bridge status
func (h *MCPHandlers) GetStatus(c *fiber.Ctx) error {
	status := fiber.Map{
		"k8sClient": h.k8sClient != nil,
	}

	if h.bridge != nil {
		bridgeStatus := h.bridge.Status()
		status["mcpBridge"] = bridgeStatus
	} else {
		status["mcpBridge"] = fiber.Map{"available": false}
	}

	return c.JSON(status)
}

// GetOpsTools returns available klaude-ops tools
func (h *MCPHandlers) GetOpsTools(c *fiber.Ctx) error {
	if h.bridge == nil {
		return c.Status(503).JSON(fiber.Map{"error": "MCP bridge not available"})
	}

	tools := h.bridge.GetOpsTools()
	return c.JSON(fiber.Map{"tools": tools})
}

// GetDeployTools returns available klaude-deploy tools
func (h *MCPHandlers) GetDeployTools(c *fiber.Ctx) error {
	if h.bridge == nil {
		return c.Status(503).JSON(fiber.Map{"error": "MCP bridge not available"})
	}

	tools := h.bridge.GetDeployTools()
	return c.JSON(fiber.Map{"tools": tools})
}

// ListClusters returns all discovered clusters with health data
func (h *MCPHandlers) ListClusters(c *fiber.Ctx) error {
	// Try MCP bridge first if available
	if h.bridge != nil {
		clusters, err := h.bridge.ListClusters(c.Context())
		if err == nil && len(clusters) > 0 {
			return c.JSON(fiber.Map{"clusters": clusters, "source": "mcp"})
		}
		log.Printf("MCP bridge ListClusters failed, falling back to k8s client: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		clusters, err := h.k8sClient.ListClusters(c.Context())
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}

		// Enrich with health data (parallel fetch)
		healthData, _ := h.k8sClient.GetAllClusterHealth(c.Context())
		healthMap := make(map[string]*k8s.ClusterHealth)
		for i := range healthData {
			healthMap[healthData[i].Cluster] = &healthData[i]
		}

		// Merge health data into clusters
		for i := range clusters {
			if health, ok := healthMap[clusters[i].Name]; ok {
				clusters[i].Healthy = health.Healthy
				clusters[i].NodeCount = health.NodeCount
				clusters[i].PodCount = health.PodCount
			}
		}

		return c.JSON(fiber.Map{"clusters": clusters, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetClusterHealth returns health for a specific cluster
func (h *MCPHandlers) GetClusterHealth(c *fiber.Ctx) error {
	cluster := c.Params("cluster")

	// Try MCP bridge first if available
	if h.bridge != nil {
		health, err := h.bridge.GetClusterHealth(c.Context(), cluster)
		if err == nil {
			return c.JSON(health)
		}
		log.Printf("MCP bridge GetClusterHealth failed, falling back: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		health, err := h.k8sClient.GetClusterHealth(c.Context(), cluster)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(health)
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetAllClusterHealth returns health for all clusters
func (h *MCPHandlers) GetAllClusterHealth(c *fiber.Ctx) error {
	// Use direct k8s client for this as it's more efficient
	if h.k8sClient != nil {
		health, err := h.k8sClient.GetAllClusterHealth(c.Context())
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"health": health})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetPods returns pods for a namespace/cluster
func (h *MCPHandlers) GetPods(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")
	labelSelector := c.Query("labelSelector")

	// Try MCP bridge first for its richer functionality
	if h.bridge != nil {
		pods, err := h.bridge.GetPods(c.Context(), cluster, namespace, labelSelector)
		if err == nil {
			return c.JSON(fiber.Map{"pods": pods, "source": "mcp"})
		}
		log.Printf("MCP bridge GetPods failed, falling back: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil && cluster != "" {
		pods, err := h.k8sClient.GetPods(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"pods": pods, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// FindPodIssues returns pods with issues
func (h *MCPHandlers) FindPodIssues(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	// Try MCP bridge first
	if h.bridge != nil {
		issues, err := h.bridge.FindPodIssues(c.Context(), cluster, namespace)
		if err == nil {
			return c.JSON(fiber.Map{"issues": issues, "source": "mcp"})
		}
		log.Printf("MCP bridge FindPodIssues failed, falling back: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allIssues []k8s.PodIssue
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					issues, err := h.k8sClient.FindPodIssues(ctx, clusterName, namespace)
					if err == nil && len(issues) > 0 {
						mu.Lock()
						allIssues = append(allIssues, issues...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"issues": allIssues, "source": "k8s"})
		}

		issues, err := h.k8sClient.FindPodIssues(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"issues": issues, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetGPUNodes returns nodes with GPU resources
func (h *MCPHandlers) GetGPUNodes(c *fiber.Ctx) error {
	cluster := c.Query("cluster")

	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allNodes []k8s.GPUNode
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					nodes, err := h.k8sClient.GetGPUNodes(ctx, clusterName)
					if err == nil && len(nodes) > 0 {
						mu.Lock()
						allNodes = append(allNodes, nodes...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"nodes": allNodes, "source": "k8s"})
		}

		nodes, err := h.k8sClient.GetGPUNodes(c.Context(), cluster)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"nodes": nodes, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetNVIDIAOperatorStatus returns NVIDIA GPU and Network operator status
func (h *MCPHandlers) GetNVIDIAOperatorStatus(c *fiber.Ctx) error {
	cluster := c.Query("cluster")

	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allStatus []*k8s.NVIDIAOperatorStatus
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					status, err := h.k8sClient.GetNVIDIAOperatorStatus(ctx, clusterName)
					if err == nil && (status.GPUOperator != nil || status.NetworkOperator != nil) {
						mu.Lock()
						allStatus = append(allStatus, status)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"operators": allStatus, "source": "k8s"})
		}

		status, err := h.k8sClient.GetNVIDIAOperatorStatus(c.Context(), cluster)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"operator": status, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetNodes returns detailed node information
func (h *MCPHandlers) GetNodes(c *fiber.Ctx) error {
	cluster := c.Query("cluster")

	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allNodes []k8s.NodeInfo
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					nodes, err := h.k8sClient.GetNodes(ctx, clusterName)
					if err == nil && len(nodes) > 0 {
						mu.Lock()
						allNodes = append(allNodes, nodes...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"nodes": allNodes, "source": "k8s"})
		}

		nodes, err := h.k8sClient.GetNodes(c.Context(), cluster)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"nodes": nodes, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// FindDeploymentIssues returns deployments with issues
func (h *MCPHandlers) FindDeploymentIssues(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allIssues []k8s.DeploymentIssue
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					issues, err := h.k8sClient.FindDeploymentIssues(ctx, clusterName, namespace)
					if err == nil && len(issues) > 0 {
						mu.Lock()
						allIssues = append(allIssues, issues...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"issues": allIssues, "source": "k8s"})
		}

		issues, err := h.k8sClient.FindDeploymentIssues(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"issues": issues, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetDeployments returns deployments with rollout status
func (h *MCPHandlers) GetDeployments(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allDeployments []k8s.Deployment
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					deployments, err := h.k8sClient.GetDeployments(ctx, clusterName, namespace)
					if err == nil && len(deployments) > 0 {
						mu.Lock()
						allDeployments = append(allDeployments, deployments...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"deployments": allDeployments, "source": "k8s"})
		}

		deployments, err := h.k8sClient.GetDeployments(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"deployments": deployments, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetServices returns services from clusters
func (h *MCPHandlers) GetServices(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allServices []k8s.Service
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					services, err := h.k8sClient.GetServices(ctx, clusterName, namespace)
					if err == nil && len(services) > 0 {
						mu.Lock()
						allServices = append(allServices, services...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"services": allServices, "source": "k8s"})
		}

		services, err := h.k8sClient.GetServices(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"services": services, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetJobs returns jobs from clusters
func (h *MCPHandlers) GetJobs(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allJobs []k8s.Job
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					jobs, err := h.k8sClient.GetJobs(ctx, clusterName, namespace)
					if err == nil && len(jobs) > 0 {
						mu.Lock()
						allJobs = append(allJobs, jobs...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"jobs": allJobs, "source": "k8s"})
		}

		jobs, err := h.k8sClient.GetJobs(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"jobs": jobs, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetHPAs returns HPAs from clusters
func (h *MCPHandlers) GetHPAs(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allHPAs []k8s.HPA
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					hpas, err := h.k8sClient.GetHPAs(ctx, clusterName, namespace)
					if err == nil && len(hpas) > 0 {
						mu.Lock()
						allHPAs = append(allHPAs, hpas...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"hpas": allHPAs, "source": "k8s"})
		}

		hpas, err := h.k8sClient.GetHPAs(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"hpas": hpas, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetConfigMaps returns ConfigMaps from clusters
func (h *MCPHandlers) GetConfigMaps(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allConfigMaps []k8s.ConfigMap
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					configmaps, err := h.k8sClient.GetConfigMaps(ctx, clusterName, namespace)
					if err == nil && len(configmaps) > 0 {
						mu.Lock()
						allConfigMaps = append(allConfigMaps, configmaps...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"configmaps": allConfigMaps, "source": "k8s"})
		}

		configmaps, err := h.k8sClient.GetConfigMaps(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"configmaps": configmaps, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetSecrets returns Secrets from clusters
func (h *MCPHandlers) GetSecrets(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allSecrets []k8s.Secret
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					secrets, err := h.k8sClient.GetSecrets(ctx, clusterName, namespace)
					if err == nil && len(secrets) > 0 {
						mu.Lock()
						allSecrets = append(allSecrets, secrets...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"secrets": allSecrets, "source": "k8s"})
		}

		secrets, err := h.k8sClient.GetSecrets(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"secrets": secrets, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetServiceAccounts returns ServiceAccounts from clusters
func (h *MCPHandlers) GetServiceAccounts(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	if h.k8sClient != nil {
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allServiceAccounts []k8s.ServiceAccount
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					serviceAccounts, err := h.k8sClient.GetServiceAccounts(ctx, clusterName, namespace)
					if err == nil && len(serviceAccounts) > 0 {
						mu.Lock()
						allServiceAccounts = append(allServiceAccounts, serviceAccounts...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"serviceAccounts": allServiceAccounts, "source": "k8s"})
		}

		serviceAccounts, err := h.k8sClient.GetServiceAccounts(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"serviceAccounts": serviceAccounts, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetPodLogs returns logs from a pod
func (h *MCPHandlers) GetPodLogs(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")
	pod := c.Query("pod")
	container := c.Query("container")
	tailLines := c.QueryInt("tail", 100)

	if cluster == "" || namespace == "" || pod == "" {
		return c.Status(400).JSON(fiber.Map{"error": "cluster, namespace, and pod are required"})
	}

	if h.k8sClient != nil {
		logs, err := h.k8sClient.GetPodLogs(c.Context(), cluster, namespace, pod, container, int64(tailLines))
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"logs": logs, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetEvents returns events from clusters
func (h *MCPHandlers) GetEvents(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")
	limit := c.QueryInt("limit", 50)

	// Try MCP bridge first
	if h.bridge != nil {
		events, err := h.bridge.GetEvents(c.Context(), cluster, namespace, limit)
		if err == nil {
			return c.JSON(fiber.Map{"events": events, "source": "mcp"})
		}
		log.Printf("MCP bridge GetEvents failed, falling back: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel with timeout
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			perClusterLimit := limit / len(clusters)
			if perClusterLimit < 10 {
				perClusterLimit = 10
			}

			// Query clusters in parallel with 5 second timeout per cluster
			var wg sync.WaitGroup
			var mu sync.Mutex
			var allEvents []k8s.Event
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					events, err := h.k8sClient.GetEvents(ctx, clusterName, namespace, perClusterLimit)
					if err == nil && len(events) > 0 {
						mu.Lock()
						allEvents = append(allEvents, events...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()

			// Sort by timestamp (most recent first) and limit total
			if len(allEvents) > limit {
				allEvents = allEvents[:limit]
			}
			return c.JSON(fiber.Map{"events": allEvents, "source": "k8s"})
		}

		events, err := h.k8sClient.GetEvents(c.Context(), cluster, namespace, limit)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"events": events, "source": "k8s", "cluster": cluster})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// GetWarningEvents returns warning events from clusters
func (h *MCPHandlers) GetWarningEvents(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")
	limit := c.QueryInt("limit", 50)

	// Try MCP bridge first
	if h.bridge != nil {
		events, err := h.bridge.GetWarningEvents(c.Context(), cluster, namespace, limit)
		if err == nil {
			return c.JSON(fiber.Map{"events": events, "source": "mcp"})
		}
		log.Printf("MCP bridge GetWarningEvents failed, falling back: %v", err)
	}

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			perClusterLimit := limit / len(clusters)
			if perClusterLimit < 10 {
				perClusterLimit = 10
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allEvents []k8s.Event
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					events, err := h.k8sClient.GetWarningEvents(ctx, clusterName, namespace, perClusterLimit)
					if err == nil && len(events) > 0 {
						mu.Lock()
						allEvents = append(allEvents, events...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()

			// Limit total
			if len(allEvents) > limit {
				allEvents = allEvents[:limit]
			}
			return c.JSON(fiber.Map{"events": allEvents, "source": "k8s"})
		}

		events, err := h.k8sClient.GetWarningEvents(c.Context(), cluster, namespace, limit)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"events": events, "source": "k8s", "cluster": cluster})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// CheckSecurityIssues returns security misconfigurations
func (h *MCPHandlers) CheckSecurityIssues(c *fiber.Ctx) error {
	cluster := c.Query("cluster")
	namespace := c.Query("namespace")

	// Fall back to direct k8s client
	if h.k8sClient != nil {
		// If no cluster specified, query all clusters in parallel
		if cluster == "" {
			clusters, err := h.k8sClient.ListClusters(c.Context())
			if err != nil {
				return c.Status(500).JSON(fiber.Map{"error": err.Error()})
			}

			var wg sync.WaitGroup
			var mu sync.Mutex
			var allIssues []k8s.SecurityIssue
			clusterTimeout := 5 * time.Second

			for _, cl := range clusters {
				wg.Add(1)
				go func(clusterName string) {
					defer wg.Done()
					ctx, cancel := context.WithTimeout(c.Context(), clusterTimeout)
					defer cancel()

					issues, err := h.k8sClient.CheckSecurityIssues(ctx, clusterName, namespace)
					if err == nil && len(issues) > 0 {
						mu.Lock()
						allIssues = append(allIssues, issues...)
						mu.Unlock()
					}
				}(cl.Name)
			}

			wg.Wait()
			return c.JSON(fiber.Map{"issues": allIssues, "source": "k8s"})
		}

		issues, err := h.k8sClient.CheckSecurityIssues(c.Context(), cluster, namespace)
		if err != nil {
			return c.Status(500).JSON(fiber.Map{"error": err.Error()})
		}
		return c.JSON(fiber.Map{"issues": issues, "source": "k8s"})
	}

	return c.Status(503).JSON(fiber.Map{"error": "No cluster access available"})
}

// CallToolRequest represents a request to call an MCP tool
type CallToolRequest struct {
	Name      string                 `json:"name"`
	Arguments map[string]interface{} `json:"arguments"`
}

// CallOpsTool calls a klaude-ops tool
func (h *MCPHandlers) CallOpsTool(c *fiber.Ctx) error {
	if h.bridge == nil {
		return c.Status(503).JSON(fiber.Map{"error": "MCP bridge not available"})
	}

	var req CallToolRequest
	if err := c.BodyParser(&req); err != nil {
		return c.Status(400).JSON(fiber.Map{"error": "invalid request body"})
	}

	result, err := h.bridge.CallOpsTool(c.Context(), req.Name, req.Arguments)
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": err.Error()})
	}

	return c.JSON(result)
}

// CallDeployTool calls a klaude-deploy tool
func (h *MCPHandlers) CallDeployTool(c *fiber.Ctx) error {
	if h.bridge == nil {
		return c.Status(503).JSON(fiber.Map{"error": "MCP bridge not available"})
	}

	var req CallToolRequest
	if err := c.BodyParser(&req); err != nil {
		return c.Status(400).JSON(fiber.Map{"error": "invalid request body"})
	}

	result, err := h.bridge.CallDeployTool(c.Context(), req.Name, req.Arguments)
	if err != nil {
		return c.Status(500).JSON(fiber.Map{"error": err.Error()})
	}

	return c.JSON(result)
}
